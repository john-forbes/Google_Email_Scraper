{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Email Scraper Tool \n",
    "Adopted from: https://towardsdatascience.com/web-scraping-to-extract-contact-information-part-1-mailing-lists-854e8a8844d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTIONS: \n",
    "    - CHANGE USER AGENT: Within the 'process' variable in the 'get_info' function, you'll see settings=('USER AGENT': 'Mozilla...\"). Change this value to your personal user agent by copying and pasting the immediate results to a google search query of \"What is my user agent?\" \n",
    "    - Run all cells until the cell starting with the \"final_result\" variable manually performing \"Shift+Return\" all the way down, or after single clicking on said cell to select it, clicking \"Run All Above\" within the \"Cell\" button in PANDAS.\n",
    "    - In 'final_results' variable using the 'get_info' method, change the string to any google search query, and the number to some value of sites to scrape.* \n",
    "    - Leave the language ('en'), & path holder ('csv') as is.\n",
    "    - Run the cell for final results. ** \n",
    "    - In the following cell containing \"to_csv\", change the path to wherever desired within the string within the \"to_csv\" call. Its base route is to your local downloads folder with a generic name. \n",
    "    - Run the cell, and access your results where directed to! \n",
    "    - Terminate the program (\"Close and Halt\" button within the \"File\" button on PANDAS) and restart it to run the script with a different query. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*: There stands an increasing run time and chance of failure due to too rapid of an API call & scraping detection software as this number increases. This will manifest as a \"ValueError\" when running the script, and when occurs is best to terminate and restart the program with a lower number of URLs to scrape as it will get stuck on this action. ITs detection as a scraper is likely as result of not paying for a USER AGENT databse & filtering through it while running. I reccomend starting with 10 to familiarize yourself with the results, and go higher over time with knowledge of the potential failure + greater run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The \"ReactorNotRestartable\" error will pop up if you run the script a second time due to an error with the Scrapy API call. The script runs once, then is required to be quit and restarted for each individual use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Email Scraper\n",
    "ACTION: Scrape Emails + URLs from Google Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTSTANDING CHALLENGES: Without paying for open source user lists, we are not able to get past all sites' scripting prevention software. The script runs successfully once, then must be terminated & restarted due to an issue with the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "# Scrapy enables us to Crawl Websites. Install with pip install scrapy\n",
    "import scrapy \n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
    "# Allows easier Google Functionality. Install with pip install google-search\n",
    "#from googlesearch import GoogleSearch\n",
    "from googlesearch import search\n",
    "# A dependancy for using Scrapy within Pandas - to avoid excess warnings & logs.\n",
    "logging.getLogger('scrapy').propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to return URLs from a search query. \n",
    "Feed in a list query, any number of results per page up to 100, and a stop value <= the number of results.\n",
    "\n",
    "WARNING: Google will throw a HTTPL Error 429 if you request too many queries at once. \n",
    "Limit is 50k per day, but 10 per second. \n",
    "Source:https://developers.google.com/analytics/devguides/config/mgmt/v3/limits-quotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_urls(query_string, stop_val,language='en'):\n",
    "\turl_list = [url for url in search(query_string, stop=stop_val)][:stop_val]\n",
    "\treturn url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex function to find emails from a site's html: 1+ letters, then an @ sign, then more letters, then a '.', then more letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_finder(html_text):\n",
    "\treturn re.findall(r'\\w+@\\w+\\.{1}\\w+', html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create the final file for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(path):    \n",
    "    with open(path, 'wb') as file: \n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy Spider to extract emails from a set of URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MailSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'email'\n",
    "    # To Avoid Denial of Crawling by Websites. Slows program down, but the reccomended course of action should Spider fail.\n",
    "    #Set to any number 2+ for best results\n",
    "    #download_delay = 2 \n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        links = LxmlLinkExtractor(allow=()).extract_links(response)\n",
    "        links = [str(link.url) for link in links]\n",
    "        links.append(str(response.url))\n",
    "        \n",
    "        for link in links:\n",
    "            yield scrapy.Request(url=link, callback=self.parse_link) \n",
    "            \n",
    "    def parse_link(self, response):\n",
    "        \n",
    "        for word in self.reject:\n",
    "            if word in str(response.url):\n",
    "                return\n",
    "            \n",
    "        html_text = str(response.text)\n",
    "        mail_list = email_finder(html_text)\n",
    "\n",
    "        dic = {'email': mail_list, 'link': str(response.url)}\n",
    "        df = pd.DataFrame(dic)\n",
    "        \n",
    "        df.to_csv(self.path, mode='a', header=False)\n",
    "        df.to_csv(self.path, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates new CSV with an empty data frame, gets Google URLs from 'get_urls' funntion, scrapes for emails, returns new CSV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(tag, n, language, path, reject=[]):\n",
    "    \n",
    "    create_file(path)\n",
    "    df = pd.DataFrame(columns=['email', 'link'], index=[0])\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    print('Collecting Google urls...')\n",
    "    google_urls = return_urls(tag, n, language)\n",
    "    \n",
    "    print('Searching for emails...')\n",
    "    #Change to your own USER AGENT by replacing the \"Mozilla...\" string with the google result to \"what is my user-agent\"\n",
    "    process = CrawlerProcess(\n",
    "    \tsettings={'USER_AGENT': 'Mozilla/5.0 (Macintosh;...',\n",
    "    \t\t\t\n",
    "    \t\t\t })\n",
    "    process.crawl(MailSpider, start_urls=google_urls, path=path, reject=reject)\n",
    "    process.start()\n",
    "    \n",
    "    #time.sleep(0.5)\n",
    "\n",
    "    #os.execl(sys.executable, sys.executable, *sys.argv)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Cleaning emails...')\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df.columns = ['email', 'link']\n",
    "    df = df[~df.email.str.contains(\"@2x.png\", na=False)]\n",
    "    df = df.drop_duplicates(subset='email')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(path, mode='w', header=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the script, use the get_info function passing in a google search for 'tag', any number fot the stop value, 'en' for language, and any unique name for the path (CSV file). Currently, the string shows an example google query for scraping 20 URLs to find therapists in Detroit. Since it is a larger query it takes a few minutes to run, and in this case yields around 1200 results (90%+ usable emails) in its CSV output. It errored scraping numerous sites which detected the scraper, but given enough time still yields said output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result =  get_info('therapists @gmail.com detroit email', 20, 'en','csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('./downloads/google_scraper_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Remarks: A known error is that emails with a '.' before the '@' sign may return a concatenated email, if an email in the resulting spreadsheet looks off - click on its subsequent link and Control-F with it to find the full email. Your quality of results can depend on a 'hit or miss' basis with the serach query. \"therapists @gmail.com new york email\" yielded 200+ emails, and depending on what sites pop up the number of scrapable emails differs greatly. For a poor set of results, switching up the query by city is a good way to try again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
